Understanding Calibration (Corrected + Organized)

1) What calibration IS
- Calibration does NOT mean “fix wrong predictions into correct ones” by changing labels.
- Calibration means: making the model’s predicted probabilities more reliable.
  Example:
  - If the model outputs 0.80 for “Edema”, then among cases where it predicts ~0.80,
    the true frequency of Edema should be close to 80%.
- Calibration is a post-processing step applied to the model’s OUTPUT scores
  (logits/probabilities), not to the model’s weights.

2) What calibration is NOT
- Calibration is NOT retraining the model on new data.
- Calibration does NOT change the model’s weights (.pth checkpoint).
- Calibration does NOT use the same training data to “correct” predictions.
- Using the same data the model trained on for calibration is usually a bad idea
  because it can hide overconfidence and inflate results.

3) Why we need a separate dataset for calibration
- A model trained on CheXpert can be miscalibrated on a different dataset (NIH)
  because of domain shift (different hospital, scanners, prevalence, labeling).
- Therefore, we use NIH as an external dataset to learn how to interpret the
  CheXpert-trained model’s outputs on unseen data.

4) Training / Validation / Test (general meaning)
- Training set: used to TRAIN model weights (gradient updates happen).
- Validation set: used to tune hyperparameters / early stopping / model selection.
  (NO training updates on validation)
- Test set: used once at the end to report final performance (NO tuning).

Important:
- In this project, we are NOT retraining the CheXpert model weights.
  So NIH splits are not “train/val/test” for weight training.
  NIH splits are used for calibration + conformal + evaluation only.

5) NIH Splits used in THIS project (recommended)
We split NIH into 3 disjoint sets:

A) NIH-Calib-Prob  (Calibration set for probabilities)
Role:
- Fit the calibration method (e.g., temperature scaling / Platt scaling).
What happens:
- Run the frozen CheXpert-trained model on NIH-Calib-Prob.
- Collect model outputs (logits/probabilities) + ground-truth labels.
- Learn calibration parameters (e.g., temperature T).
Output:
- A saved calibrator file (e.g., temps.json / temps.pt).

B) NIH-Calib-Conf  (Calibration set for conformal prediction)
Role:
- Fit conformal thresholds (quantiles) to create prediction sets / triage outputs.
What happens:
- Run the frozen model on NIH-Calib-Conf.
- Apply the learned calibration from NIH-Calib-Prob (important).
- Compute conformal nonconformity scores.
- Choose thresholds for a target error rate α (e.g., α=0.1 for 90% coverage).
Output:
- A saved conformal config file (thresholds per label).

C) NIH-Test  (Final evaluation set)
Role:
- Final, untouched evaluation of:
  - calibration quality (ECE, Brier, NLL, reliability plots)
  - conformal performance (coverage, set size, abstention rate)
What happens:
- NO fitting/tuning here.
- Only run inference + apply calibration + apply conformal thresholds.
Output:
- Final results reported in the dissertation.

6) Summary in one line
- CheXpert: used to obtain the trained model checkpoint.
- NIH-Calib-Prob: fits probability calibration.
- NIH-Calib-Conf: fits conformal thresholds.
- NIH-Test: final evaluation (no fitting).

Key takeaway:
Calibration = “make probabilities honest”, not “change model predictions to be correct”.
Conformal = “use calibrated outputs to produce uncertainty-aware sets/triage with a target coverage”.